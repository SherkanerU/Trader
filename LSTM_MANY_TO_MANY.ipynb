{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_MANY_TO_MANY",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SherkanerU/Trader/blob/master/LSTM_MANY_TO_MANY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PCcDHqukbHd"
      },
      "source": [
        "1. Don't edit this, it is a template\n",
        "2. Save a copy to your drive and move it into the shared folder `coinbase`\n",
        "3. Experiment\n",
        "\n",
        "Things to try:\n",
        "- Decay the learning rate over time\n",
        "- Smaller batches\n",
        "- Regularization and dropout\n",
        "- Normalize data within range (-1,1) instead of (0,1)\n",
        "- Try GRU/simple RNN\n",
        "- Binary classification, multiclass classification\n",
        "- Do a random/grid search through hyperparams?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_7bsTka3KGy"
      },
      "source": [
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import time\n",
        "\n",
        "import google.colab as colab\n",
        "\n",
        "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgZzIHQ4-CMN"
      },
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcIF7ZxtGgAI"
      },
      "source": [
        "from sqlalchemy import create_engine\r\n",
        "\r\n",
        "!pip3 install pymysql\r\n",
        "import pymysql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itORdsk15som"
      },
      "source": [
        "# dev = torch.device('cpu') \n",
        "dev = torch.device('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPZTrHPDDpQ5"
      },
      "source": [
        "# Authenticate with gcloud\r\n",
        "from google.colab import auth\r\n",
        "auth.authenticate_user()\r\n",
        "\r\n",
        "project_id = 'colman-coin'\r\n",
        "!gcloud config set project {project_id}\r\n",
        "\r\n",
        "!gsutil cp -r gs://keys_please/colab_ssh/.ssh/ /root/\r\n",
        "\r\n",
        "!chmod 700 ~/.ssh\r\n",
        "!chmod 644 ~/.ssh/known_hosts\r\n",
        "!chmod 600 ~/.ssh/id_rsa\r\n",
        "!chmod 644 ~/.ssh/id_rsa.pub\r\n",
        "!ssh -T git@github.com"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSoNuEY83xmI"
      },
      "source": [
        "# Get the preprocessing code from github\n",
        "!git config --global user.email \"connorfinucane1@gmail.com\"\n",
        "!git config --global user.name \"sherkanerU\"\n",
        "!rm -r coin_trader/\n",
        "!git clone git@github.com:SherkanerU/coin_trader.git\n",
        "!cd coin_trader && git checkout beenis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUNskNhKpEtz"
      },
      "source": [
        "#need to do this because the jupyer notebook kernel will not see the file\r\n",
        "#having changed on disk otherwise\r\n",
        "import importlib\r\n",
        "import coin_trader.src.datasets.MaxReturnDataset as mr\r\n",
        "importlib.reload(mr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVzqO33oSrQx"
      },
      "source": [
        "bank = ['btc', 'eth', 'bnb', 'neo', 'ltc', 'qtum', 'ada', 'xrp', 'eos',\r\n",
        " 'tusd', 'xlm', 'ont', 'trx', 'etc', 'icx', 'vet', 'pax', 'usdc', 'link', 'waves',\r\n",
        " 'btt', 'hot', 'zil', 'zrx', 'bat', 'xmr', 'zec', 'iost', 'dash', 'nano', 'omg',\r\n",
        " 'theta', 'enj', 'matic', 'atom', 'algo', 'doge', 'busd', 'xtz', 'ren', 'rvn',\r\n",
        " 'hc', 'hbar', 'stx', 'mco', 'bch', 'ftt', 'bts', 'lsk', 'data', 'xzc', 'hive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tt1Tugmtniv"
      },
      "source": [
        "This is a messy gcloud mysql hack forgive me jesus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECTdEizmHRnx"
      },
      "source": [
        "!nohup gcloud sql connect coin-data-sql --user=root > /dev/null 2> /dev/null &\r\n",
        "!sleep 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwfD9U1GQBiQ"
      },
      "source": [
        "sql_engine = create_engine('mysql+pymysql://root:crypto@35.231.47.132/coins', pool_recycle=3600)\r\n",
        "Connection = sql_engine.connect()\r\n",
        "print(\"connected\")\r\n",
        "coin_dict = {}\r\n",
        "for coin in bank:\r\n",
        "  print(f\"downloading {coin}\")\r\n",
        "  coin_dict[coin] = pd.read_sql(f\"select * from {coin} order by timestamp asc\", Connection)[[\"timestamp\", \"market\"]]\r\n",
        "Connection.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSoPyGjvSmhi"
      },
      "source": [
        "#just a sanity check\r\n",
        "plt.plot(coin_dict[\"eth\"][\"market\"])\r\n",
        "import time\r\n",
        "time.time() - coin_dict[\"neo\"][\"timestamp\"].tail(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MjHP8hlkAXi"
      },
      "source": [
        "coin_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWfPQ5LvcaOX"
      },
      "source": [
        "eth_lookahead = mr.add_lookahead_feature(\r\n",
        "    lambda x: x.max(),\r\n",
        "    coin_dict[\"bch\"],\r\n",
        "    lookahead = 4*24*12,\r\n",
        "    col = \"market\",\r\n",
        "    feature = \"4_day_max\",\r\n",
        "    shave = True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIVUA_jjlGhR"
      },
      "source": [
        "plt.plot(coin_dict[\"bch\"][\"market\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBoUOfrXc5Jz"
      },
      "source": [
        "import time\r\n",
        "eth_restr = eth_lookahead[eth_lookahead[\"timestamp\"] >= time.time() - 30*24*60*60]\r\n",
        "plt.plot((eth_restr[\"4_day_max\"] - eth_restr[\"market\"])/eth_restr[\"market\"] )\r\n",
        "#plt.plot(np.zeros(eth_lookahead[\"2_day_max\"].size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_JpyHQvMZqY"
      },
      "source": [
        "Hyperparameter Search:\n",
        "- lstm_dim: [50, 350]\n",
        "- lstm_layers: [1,2]\n",
        "- dense_dim: [128, 8192]\n",
        "- num_dense: [1,6]\n",
        "- lookback: [50, 500]\n",
        "- weight_decay: [1e-5, 5e-3]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR3mZItVKfVu"
      },
      "source": [
        "# Define some variables\n",
        "lstm_dim = 256\n",
        "num_lstm_layers = 2\n",
        "dense_dim = 256\n",
        "num_dense_layers = 1\n",
        "output_dim = 1\n",
        "\n",
        "lookback = 512\n",
        "obs_ahead = 7*24*12\n",
        "test_size = 14*24*12\n",
        "max_size = 120*24*12\n",
        "coin = \"xlm\"\n",
        "\n",
        "lookahead_func = lambda x: x.max()\n",
        "# lookahead_func = lambda x: x.quantile(.9)\n",
        "\n",
        "dev = torch.device('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYEp7qK65gn0"
      },
      "source": [
        "#be careful to reset these\n",
        "x_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "y_scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "# Get the dataset, pass in your feature scalers (keep them around for the test set)\n",
        "train = mr.GeneralizedDataset(\n",
        "    coin = coin,\n",
        "    coin_dict = coin_dict,\n",
        "    lookahead_func = lookahead_func,\n",
        "    max_size=max_size,\n",
        "    lookback = lookback,\n",
        "    obs_ahead = obs_ahead,\n",
        "    t=\"train\",\n",
        "    test_size = test_size,\n",
        "    x_scaler=x_scaler, \n",
        "    y_scaler=y_scaler,\n",
        "    device = dev,\n",
        "    lazy = True,\n",
        "    post_lambda = lambda df: (df[\"temp_max\"] - df[\"market\"])/df[\"market\"])\n",
        "\n",
        "test = mr.GeneralizedDataset(\n",
        "    coin = coin,\n",
        "    coin_dict = coin_dict,\n",
        "    lookahead_func = lookahead_func,\n",
        "    max_size=max_size,\n",
        "    lookback = lookback,\n",
        "    obs_ahead = obs_ahead,\n",
        "    t=\"test\",\n",
        "    test_size = test_size,\n",
        "    x_scaler=x_scaler, \n",
        "    y_scaler=y_scaler,\n",
        "    device = dev,\n",
        "    lazy=True,\n",
        "    post_lambda = lambda df: (df[\"temp_max\"] - df[\"market\"])/df[\"market\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYCT-sBTOHTo"
      },
      "source": [
        "train[range(10000, 10050)][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzIREQ9gPLcs"
      },
      "source": [
        "train[range(10000, 10050)][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2zgWvtsPueX"
      },
      "source": [
        "train[range(10000, 10050)][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlUYSnLdP46d"
      },
      "source": [
        "train[range(10000, 10050)][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "419Nl5_lTxas"
      },
      "source": [
        "# Define your model\n",
        "class colmGRU(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout):\n",
        "    super(colmGRU, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=dev).requires_grad_()\n",
        "    out, (hn) = self.gru(x, (h0.detach()))\n",
        "    out = self.fc(out[:, -1, :])\n",
        "    return out\n",
        "class colmLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, lstm_dim, dense_dim, num_lstm_layers,\n",
        "               num_dense_layers, output_dim, dropout):\n",
        "    super(colmLSTM, self).__init__()\n",
        "    self.lstm_dim = lstm_dim\n",
        "    self.num_lstm_layers = num_lstm_layers\n",
        "    self.lstm = nn.LSTM(input_dim, lstm_dim, num_lstm_layers, batch_first=True, dropout=dropout)\n",
        "    self.fcs = []\n",
        "    dense_in_dim = lstm_dim\n",
        "    for i in range(num_dense_layers):\n",
        "      self.fcs.append(nn.Linear(dense_in_dim, dense_dim))\n",
        "      dense_in_dim = dense_dim\n",
        "    self.fcs = nn.ModuleList(self.fcs)\n",
        "    self.regress = nn.Linear(dense_in_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_lstm_layers, x.size(0), self.lstm_dim, device=dev).requires_grad_()\n",
        "    c0 = torch.zeros(self.num_lstm_layers, x.size(0), self.lstm_dim, device=dev).requires_grad_()\n",
        "    out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "    out = out[:,-1,:]\n",
        "    for fc in self.fcs:\n",
        "      out = fc(out)\n",
        "      out = nn.ReLU()(out)\n",
        "    out = self.regress(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UouzG1IKoT4Y"
      },
      "source": [
        "a = torch.from_numpy(np.array([[0,1], [1,2], [0,2]]))\r\n",
        "b = torch.from_numpy(np.array([1,4,9]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeN_0Ujmo2Ag"
      },
      "source": [
        "print(a.shape)\r\n",
        "print(b.shape)\r\n",
        "type(a[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmqmUvN7mell"
      },
      "source": [
        "def gaussian_loss(val, obs):\r\n",
        "  pi = torch.tensor(torch.pi)\r\n",
        "  factor = 1/((val[:,1] * torch.sqrt(2*pi)))\r\n",
        "  arg = (((val[:,0] - obs)*(val[:,0] - obs))/(2*(val[:,1]*val[:,1])))\r\n",
        "  return torch.mean(-factor*torch.exp(-arg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5me2PLlqMO9"
      },
      "source": [
        "gaussian_loss(a,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGNaEIF5Twt9"
      },
      "source": [
        "dropout = 0\n",
        "learning_rate = .002\n",
        "weight_decay = 5e-3\n",
        "# weight_decay = 0\n",
        "\n",
        "# Create your model\n",
        "input_dim = train[0][0].shape[1]\n",
        "model = colmLSTM(input_dim=input_dim, lstm_dim=lstm_dim, dense_dim=dense_dim,\n",
        "                 num_lstm_layers=num_lstm_layers, num_dense_layers=num_dense_layers,\n",
        "                 output_dim=output_dim, dropout=dropout).to(dev)\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "# criterion = torch.nn.L1Loss(reduction='mean')\n",
        "#criterion = gaussian_loss\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=4, verbose=True)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gscE-KdsUfT_"
      },
      "source": [
        "\n",
        "num_epochs = 25\n",
        "batch_size = 128\n",
        "\n",
        "# Train your model\n",
        "import time\n",
        "import sys\n",
        "start_time = time.time()\n",
        "train_ldr = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "test_ldr = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "train_eval_hist = []\n",
        "test_eval_hist = []\n",
        "\n",
        "num_batches = int(np.ceil(train.x_data.shape[0]/batch_size))\n",
        "for t in range(num_epochs):\n",
        "  print('\\n===================================')\n",
        "  print(\"Epoch = \" + str(t))\n",
        "  sys.stdout.flush()\n",
        "  num_batches = 0\n",
        "  epoch_loss = 0\n",
        "  \n",
        "  pbar = tqdm(train_ldr)\n",
        "  for batch in pbar:\n",
        "    X, y = batch\n",
        "\n",
        "    batch_pred = model(X)\n",
        "\n",
        "    loss = criterion(batch_pred.squeeze(), y)\n",
        "    num_batches += 1\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    pbar.set_description(f'Batch Loss: {loss.item():.4f}')\n",
        "    \n",
        "  avg_loss = epoch_loss/num_batches\n",
        "  print(f'\\nEpoch Loss = {avg_loss}')\n",
        "  train_eval_hist.append(avg_loss)\n",
        "  sys.stdout.flush()\n",
        "  sched.step(avg_loss)\n",
        "\n",
        "  test_preds = np.zeros(len(test))\n",
        "  test_expect = test[range(len(test))][1].cpu()\n",
        "  with torch.no_grad():\n",
        "    for idx, batch in enumerate(test_ldr):\n",
        "      X = batch[0]\n",
        "      batch_preds = model(X)\n",
        "      test_preds[idx*batch_size:idx*batch_size+X.shape[0]] = batch_preds.squeeze().cpu()\n",
        "    \n",
        "  test_preds = y_scaler.inverse_transform(test_preds.reshape(-1,1))\n",
        "  test_expect = y_scaler.inverse_transform(test_expect.reshape(-1,1))\n",
        "  test_MSE = np.mean((test_preds - test_expect)**2)\n",
        "  print(f'Test MSE: {test_MSE}')\n",
        "  test_eval_hist.append(test_MSE)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwaAMKcbleb7"
      },
      "source": [
        "  plt.plot(train_eval_hist[2:], label='train Loss')\n",
        "plt.plot(test_eval_hist[2:], label='test MSE')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inOn1stOjMTS"
      },
      "source": [
        "train_batch = 256\n",
        "# Evaluate model on training data\n",
        "train_pred_ldr = DataLoader(train, batch_size=train_batch, shuffle=False)\n",
        "y_pred = np.zeros(len(train))\n",
        "y_expect = train[range(len(train))][1].cpu()\n",
        "with torch.no_grad():\n",
        "  for idx, batch in enumerate(train_pred_ldr):\n",
        "    X = batch[0]\n",
        "    pred_train = model(X)\n",
        "    y_pred[idx*train_batch:idx*train_batch + X.shape[0]] = pred_train.squeeze().cpu()\n",
        "\n",
        "y_pred = y_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "plt.plot(y_scaler.inverse_transform(y_expect.reshape(-1,1)), label='expected')\n",
        "plt.plot(y_pred, label = \"predicted\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfFtM4r27MHY"
      },
      "source": [
        "plt.plot(train.coin_df[\"returns\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FavGS3iSmN26"
      },
      "source": [
        "test_preds = np.zeros(len(test))\n",
        "test_expect = np.zeros(len(test))\n",
        "with torch.no_grad():\n",
        "  for idx, batch in enumerate(test_ldr):\n",
        "    X, y = batch\n",
        "    batch_preds = model(X)\n",
        "    test_preds[idx*batch_size:idx*batch_size + X.shape[0]] = batch_preds.squeeze().cpu()\n",
        "    test_expect[idx*batch_size:idx*batch_size + X.shape[0]] = y.squeeze().cpu()\n",
        "\n",
        "test_preds = y_scaler.inverse_transform(test_preds.reshape(-1,1))\n",
        "test_expect = y_scaler.inverse_transform(test_expect.reshape(-1,1))\n",
        "plt.plot(test_expect, label='expected')\n",
        "plt.plot(test_preds, label = \"predicted\")\n",
        "plt.legend()\n",
        "\n",
        "test_MSE = np.mean(np.abs(test_preds - test_expect))\n",
        "print(test_MSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnkmWsLK5hSc"
      },
      "source": [
        "plt.plot(coin_dict[coin][\"market\"].tail(8000).values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtNRO9QASa7"
      },
      "source": [
        "coin_dict[coin][\"market\"].tail(1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}